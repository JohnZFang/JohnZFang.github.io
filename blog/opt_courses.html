<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title>Zhong Fang</title>
<!--     (English Approximation: drong) -->

    <link rel="stylesheet" href="stylesheets/styles.css">
    <link rel="stylesheet" href="stylesheets/pygment_trac.css">
    <meta name="viewport" content="width=device-width">

    <link rel="icon" type="image/x-icon" href="https://JohnZFang.github.io/photo/icon.png"/>

    <script>MathJax = {tex: {inlineMath: [['$', '$'],['$$', '$$'], ['\\(', '\\)']]}}</script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
    

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
  </head>
  <body>
    <div class="wrapper">
      <header>
        <h1><a href="https://JohnZFang.github.io">Zhong Fang</a></h1>
           <p><a href="https://JohnZFang.github.io"><img src="https://JohnZFang.github.io/photo/new_photo.jpeg" width="220"/></a><br>
             Contact: <a href="mailto:z4fang@uwaterloo.ca">z4fang [at] uwaterloo.ca</a><br>
            [<a href="https://JohnZFang.github.io/file/Zhong_Fang_CV.pdf">CV</a>] [<a href="https://JohnZFang.github.io/file/transcript.pdf">transcript</a>]</p>
              
<!--         <p>A Theme for GitHub Pages</p> -->
        

        <p>News:<br>
          <small><b>2024.07</b> Our work, Hybrid State Space and Frequency Domain System Level Synthesis for Sparsity-Promoting $\mathcal{H}_2/\mathcal{H}_\infty$ Control Design, 
            was accepted by CDC.<br>
          <b>2024.07</b> Attended IEEE American Control Conference (ACC) as a student volunteer.<br>
          <b>2023.09</b> Joined the Dynamics, Control, and Optimization of Complex Systems (DOCS) Group at University of Waterloo as a Master student in Fall'23.<br> 
          <b>2023.06</b> Graduated from Wuhan University!<br> 
          </small>
        </p>
<!--        <a href="mailto:z4fang@uwaterloo.ca"> <p class="view">  <small>orderedlist/minimal</small></a>  -->
<!--         <ul>
          <li><a href="https://github.com/orderedlist/minimal/zipball/master">Download <strong>ZIP File</strong></a></li>
          <li><a href="https://github.com/orderedlist/minimal/tarball/master">Download <strong>TAR Ball</strong></a></li>
          <li><a href="http://github.com/orderedlist/minimal">Fork On <strong>GitHub</strong></a></li>
        </ul> -->
      </header>
      <section>
        <h1>What I Learned from Three Optimization Courses</h1>
        <p><small>2024-08-23</small><br>
          by Zhong Fang</p>

<p>I've met all of my master's course requirements, congratulations!</p>
<p>These three graduate optimization courses inspired me a lot, they have different styles but all of them are depicting different branches of optimization.</p> 


<h2>CO 602: Introduction of Optimization</h2>

<p>It encompasses linear and integer programming, convex optimization and Lagrangian function. Basic definitions and theorems for optimization are built first and then the 
  large number of proofs are prepared while diving into the simplex method. Classic convex optimization concepts are conveyed in the rest of the course. 
This course is from the <a href="https://uwaterloo.ca/combinatorics-and-optimization/">Department of Combinatorics and Optimization</a>, which inherits the rigorously theoretical genre. 
  Detailed convergence analysis is developed for a specific method-Projection Gradient Method, and is highly involved.
Duality and Lagrangian are accentuated as well. Farkas' lemma, which appeared once in Linear Programming, are recalled to complete the proof for KKT conditions for more general optimization problems. 
  At the end of the course, I have two questions:
</p>
<ol type="I">
        <li>Now I have built a foundation for optimization, but there are (mainly) three optimization methods introduced in this course. 
          I believe there are more methods that were not covered (based on my undergraduate courses) and It would be useful to evoke them.
          </li>
<li>As I stated above, the convergence analysis for the Projection Gradient Method (PGM) is complex, just the thought of guaranteeing it for every method is daunting. 
  Not to mention probably countless methods on the waiting list. Is it possible to reconcile a uniform convergence analysis?
  </li>
</ol>
<p>First one is caught in SYDE 632 quickly.</p>

   <h2>SYDE 632: Optimization Methods</h2>
<p>This course is definitely focusing more on methods as opposed to theory compared to CO 602 (you can see from the course name). Classic optimization methods are unfolded in this course, 
  verification is valued including optimality criteria for unconstrained and constrained multivariable problems. 
</p>
<p>It starts from the single-variable optimization methods first, the Fibonacci method and the golden section method can be applied for unimodal function, 
  the drawback is obviously the slow convergence rate. We can't wait to think about other efficient methods despite the need to restrict our functions in a somewhat narrow range (but still applicable). 
  The intuition is natural to the gradient of objective function based on Fermat's theorem in CO 602. 
</p>
<p>If the objective function is differentiable, gradient-based techniques can either transfer an optimization problem to a root-finding problem or derive optimization algorithms. 
  There are representative root-finding methods such as secant method and bisection method (the latter is used for finding suitable parameters in my research), 
  while gradient descent method and (Quasi) Newton's method stand for iterative optimization methods. They work well in practice, things would change if we want to add some constraints though. 
  Recalling from CO 602, constrained optimization relies on the Lagrange Multipliers Method also referred to at the end of the course.
</p>
<p>Implementation of these methods is mostly concerned in this course, optimality conditions are highlighted particularly. An extra accomplishment in this course is overcoming my stage fright. 
Classic algorithms and their derivations are suitable for retrieving as a toolbox but are not convincing to follow where the convergence guarantees are ignored. 
  And we all know it's necessary when we expect to receive reasonable solutions when applying these algorithms to real-world problems. 
</p>
<p>These classic optimization methods look good, but conservativeness appears gradually as optimization problems are involved in a widely-existing large network system.</p>


        <h2>ECE 700: Distributed Optimization</h2>


<p>Before we move towards this course, there are some intractable problems faced in my research, which can also be seen as the drawbacks of some classic optimization methods. 
  Implicit conservation in the aforementioned gradient-based techniques is the requirement for differentiability, which is not a practical assumption in many cases.
It's time to reconstruct the skeleton of the convex optimization as a preparation for some novel distributed optimization algorithms. 
  </p>
<p>Basic knowledge is evoked from CO 602 and SYDE 632. For instance, the beginning of this course is Lagrangian conditions, where KKT conditions, complementary slackness, 
  duality, and Slater conditions are encapsulated (and slightly extended) in this course. The foundation has already been constructed from CO 602 so even though it's not a keypoint in 
  this course we have a more concrete understanding. At the same time, some novel definitions and theorems are introduced (for me at least), like subgradients, resolvents and proximals, to do so, 
  the scope of the former courses is extended to a more comprehensive version. For example, Gradient descent method is also recalled but based on subgradients now.
</p>
<p>Question II is not forgotten, all algorithms in this course are guaranteed by the convergence theorem of Fixed Point Iteration (FPI), that is, $$x^{k+1}=T(x^k),$$
which transfers deliberate convergence analysis in PGM in CO 602 to a verification of the operator (PGM is factly a simplification of Proximal Gradient in this course). 
  And every algorithm cascades from part of the previous one maintains the coherence and inspires us to derive some algorithms on our own.
</p>
<p>Consensus is an essential technique in this course, a more general occasion, consensus with local variables is also considered in this course whichâ€¦.
The Laplacian-based mixing matrix plays the role of communication in our distributed algorithms.
</p>
<p>It also purposely encourages me to implement distributed algorithms in real engineering problems, you can find brief info here. 
        Their background covers robotics and sensor network, which convinces me the prospective applicability will be manifested soon in upcoming research problems involving distributed systems. 
</p>
        

        <h2>Last Words</h2>
        
        <p>Optimization is one of my favorite intersections between Math and Engineering, it's always enjoyable to learn new knowledge and see them realized in my research. 
          I appreciate that I have met these three professors designing the above optimization courses and give us a range of freedom for exploring our real interests. 
          I can't wait to dive into the deeper optimization field.
        </p>
     

        <p><a href="https://JohnZFang.github.io/blog/blog_posts.html">Back to blog</a></p>
        
      </section>
<!--       <footer>
        <p>This project is maintained by <a href="http://github.com/orderedlist">Steve Smith</a></p> 
        <p><small>Hosted on GitHub Pages &mdash; Theme by <a href="https://github.com/orderedlist">orderedlist</a></small></p>
      </footer> -->
    </div>
    <script src="javascripts/scale.fix.js"></script>
  </body>
</html>
